{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "\n",
    "# Suppress output of following line and do not output True or False\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the training data from the given .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset and remove non-utf-8 characters\n",
    "df = pd.read_csv('../Datasets/train.csv', encoding='cp1252')\n",
    "df.columns = ['body', 'subreddit']\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "df['body'] = df['body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before pre-processing the data, it can be helpful to identify the characters we are dealing with in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   character  frequency\n",
      "0                 59253\n",
      "1          e      34050\n",
      "2          t      24093\n",
      "3          a      21378\n",
      "4          o      20234\n",
      "..       ...        ...\n",
      "88         œ          2\n",
      "89         °          2\n",
      "90         ã          1\n",
      "91         á          1\n",
      "92         ë          1\n",
      "\n",
      "[93 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the frequency of each appearance of a character in the dataset\n",
    "def find_frequency(df):\n",
    "    frequency = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for character in row['body']:\n",
    "            if character in frequency:\n",
    "                frequency[character] += 1\n",
    "            else:\n",
    "                frequency[character] = 1\n",
    "    return frequency\n",
    "\n",
    "# Make function to create pandas dataframe of frequency of each character\n",
    "def make_frequency_df(frequency):\n",
    "    freq_df = pd.DataFrame.from_dict(frequency, orient='index', columns=['frequency'])\n",
    "    freq_df = freq_df.sort_values(by=['frequency'], ascending=False)\n",
    "    freq_df['character'] = freq_df.index\n",
    "    freq_df = freq_df.reset_index(drop=True)\n",
    "    freq_df = freq_df[['character', 'frequency']]\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "freq = find_frequency(df)\n",
    "freq_df = make_frequency_df(freq)\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is quite a distribution of characters here. We are going to try and keep as many as possible, but also try to align things like apostrophes that have different representations in different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align encodings\n",
    "df['body'] = df['body'].str.replace('“', '\"')\n",
    "df['body'] = df['body'].str.replace('”', '\"')\n",
    "df['body'] = df['body'].str.replace('’', \"'\")\n",
    "df['body'] = df['body'].str.replace('‘', \"'\")\n",
    "df['body'] = df['body'].str.replace('—', '-')\n",
    "df['body'] = df['body'].str.replace('–', '-')\n",
    "df['body'] = df['body'].str.replace('\\n', ' ')\n",
    "df['body'] = df['body'].str.replace('/', ' ')\n",
    "df['body'] = df['body'].str.replace('#x200b', ' ')\n",
    "df['body'] = df['body'].str.replace('-', ' ')\n",
    "\n",
    "# Remove basic punctuation\n",
    "translator = str.maketrans('', '', '<>\"°œ!\\()*+,.:;=?[\\\\]^_`{|}~1234567890')\n",
    "df['body'] = df['body'].str.translate(translator)\n",
    "\n",
    "# Replace accented characters with unaccented characters\n",
    "translator = str.maketrans('àáâãäåçèéêëìíîïñòóôõöùúûüýÿ', 'aaaaaaceeeeiiiinooooouuuuyy')\n",
    "df['body'] = df['body'].str.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-examine the frequency of each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   character  frequency\n",
      "0                 62727\n",
      "1          e      35451\n",
      "2          t      24093\n",
      "3          a      21712\n",
      "4          o      20262\n",
      "5          i      19139\n",
      "6          n      19085\n",
      "7          s      18262\n",
      "8          r      16288\n",
      "9          l      12045\n",
      "10         u      10417\n",
      "11         h      10076\n",
      "12         d       9608\n",
      "13         c       8282\n",
      "14         m       6890\n",
      "15         p       6748\n",
      "16         g       5190\n",
      "17         y       5031\n",
      "18         f       4831\n",
      "19         w       4116\n",
      "20         b       4001\n",
      "21         v       3329\n",
      "22         '       2085\n",
      "23         k       2034\n",
      "24         q        991\n",
      "25         j        951\n",
      "26         x        643\n",
      "27         z        212\n",
      "28         %        127\n",
      "29         …         40\n",
      "30         $         37\n",
      "31         &         30\n",
      "32         £         26\n",
      "33                   16\n",
      "34         #         15\n",
      "35         «          9\n",
      "36         »          9\n",
      "37         €          8\n",
      "38         @          7\n"
     ]
    }
   ],
   "source": [
    "freq_aligned = find_frequency(df)\n",
    "freq_df_aligned = make_frequency_df(freq_aligned)\n",
    "print(freq_df_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preprocessing is helpful prior to tokenization. This includes expanding contracted words and removing stop-words (a, an, the) in both English and French, since we are dealing with cities like Toronto, Montreal, and Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with their lemmings\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_verbs(text):\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "\n",
    "def lemmatize_nouns(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_nouns(x.split()))\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_verbs(x))\n",
    "\n",
    "# Reconcatenate the words into a string\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "stop_words = set(stopwords.words('french'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample output after all pre-processing can be observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another showcase shortsighted stupidity enable mode transportation get car street people can't live without car actually beneficial many people possible drive traffic hard understand really…\n"
     ]
    }
   ],
   "source": [
    "print(df['body'][32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_freq(subreddit, df):\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    sub_word_index = {}\n",
    "    \n",
    "    for row in subreddit_df['body']:\n",
    "        for word in row.split():\n",
    "            if word in sub_word_index:\n",
    "                sub_word_index[word] += 1\n",
    "            else:\n",
    "                sub_word_index[word] = 1\n",
    "\n",
    "    # Sort the dictionary by value\n",
    "    sub_word_index = dict(sorted(sub_word_index.items(), key=lambda item: item[1], reverse=True))\n",
    "    return list(sub_word_index.items())\n",
    "\n",
    "subreddits = ['Toronto', 'Montreal', 'Paris', 'London']\n",
    "subdict = {}\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    subdict[subreddit] = get_highest_freq(subreddit, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def remove_uncommon_words(subreddit):\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    \n",
    "    # Build a vocabulary of words and how many samples they appear in\n",
    "    vocab = {}\n",
    "    for row in subreddit_df['body']:\n",
    "        for word in row.split():\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "\n",
    "    # Remove words that appear in less than 1% of the samples\n",
    "    for word in list(vocab):\n",
    "        if vocab[word] < 0.01 * len(subreddit_df):\n",
    "            del vocab[word]\n",
    "\n",
    "    # Remove all words that are not in the vocabulary\n",
    "    subreddit_df['body'] = subreddit_df['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "    return subreddit_df\n",
    "\n",
    "toronto_df = remove_uncommon_words('Toronto')\n",
    "montreal_df = remove_uncommon_words('Montreal')\n",
    "paris_df = remove_uncommon_words('Paris')\n",
    "london_df = remove_uncommon_words('London')\n",
    "\n",
    "new_df = pd.concat([toronto_df, montreal_df, paris_df, london_df])\n",
    "\n",
    "# Build vocabulary of words in new_df\n",
    "vocab = []\n",
    "for row in new_df['body']:\n",
    "    for word in row.split():\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "# Get the frequency of each word in the vocabulary (how many samples it appears in)\n",
    "new_subdict = {}\n",
    "for subreddit in subreddits:\n",
    "    new_subdict[subreddit] = get_highest_freq(subreddit, new_df)\n",
    "for key in new_subdict:\n",
    "    new_subdict[key] = [item[0] for item in new_subdict[key]]\n",
    "\n",
    "# Get aggregate index of words in the vocabulary\n",
    "agg_index = {}\n",
    "for word in vocab:\n",
    "    agg_index[word] = 0\n",
    "    for key in new_subdict:\n",
    "        if word in new_subdict[key]:\n",
    "            agg_index[word] += new_subdict[key].index(word)\n",
    "        else:\n",
    "            agg_index[word] += len(new_subdict[key])\n",
    "\n",
    "# Sort the dictionary by value\n",
    "agg_index = dict(sorted(agg_index.items(), key=lambda item: item[1], reverse=False))\n",
    "\n",
    "# Remove the most common words from new_df\n",
    "for word in list(agg_index)[:250]:\n",
    "    for sample in new_df['body']:\n",
    "        if word in sample.split():\n",
    "            new_df['body'] = new_df['body'].replace(sample, sample.replace(word + ' ', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaned training set is saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df.to_csv('../Datasets/train_cleaned.csv', index=False)\n",
    "new_df.to_csv('../Datasets/train_cleaned2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
