{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Suppress output of following line and do not output True or False\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the training data from the given .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('../Datasets/train.csv', encoding='cp1252')\n",
    "df.columns = ['body', 'subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the data before generating splits immediately to prevent data leakage later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import prep_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = prep_data(df) # Custom preprocessing function\n",
    "\n",
    "# Split dataset into training and testing to prevent any data leakage\n",
    "X_df = df['body']\n",
    "y_df = df['subreddit']\n",
    "y_df = y_df.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, stratify=y_df, test_size=0.20, random_state=42)\n",
    "\n",
    "# Package X_test and y_test into a dataframe\n",
    "y_test = y_test.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Package X_train and y_train back into a dataframe\n",
    "y_train = y_train.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "df = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that appear in less than 1% of the samples are removed. This reduces the total vocabulary in the dataset by about half. Next, the top words that are most common among every subreddit are removed as they muddy the waters when it comes to classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from prep import remove_uncommon_words, remove_common_words, get_highest_freq, build_vocab\n",
    "\n",
    "subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "\n",
    "df = remove_common_words(df, subreddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do an MI study on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import get_term_freq, get_mutual_information\n",
    "\n",
    "vocab = build_vocab(df)\n",
    "\n",
    "classes = df['subreddit'].unique()\n",
    "class_counts = df['subreddit'].value_counts().to_numpy()\n",
    "\n",
    "term_freq = {}\n",
    "for subreddit in classes:\n",
    "    term_freq[subreddit] = get_term_freq(df, subreddit, vocab)\n",
    "    \n",
    "# Make a dataframe of the term frequencies\n",
    "term_freq_df = pd.DataFrame.from_dict(term_freq, orient='index')\n",
    "term_freq_df = term_freq_df.transpose()\n",
    "\n",
    "# Create a dataframe of the mutual information\n",
    "MI = get_mutual_information(term_freq_df, class_counts)\n",
    "MI_df = pd.DataFrame(MI, columns=['MI'])\n",
    "MI_df['word'] = list(vocab)\n",
    "MI_df = MI_df.sort_values(by=['MI'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the top words based on MI\n",
    "MI_N = 3750\n",
    "MI_df_top = MI_df.head(MI_N)\n",
    "top_words = MI_df_top['word'].tolist()\n",
    "\n",
    "# Create a new dataframe with only the top words\n",
    "top_df = df.copy()\n",
    "top_df['body'] = top_df['body'].apply(lambda x: ' '.join([word for word in x.split() if word in top_words]))\n",
    "\n",
    "# Remove samples with no words\n",
    "top_df = top_df[top_df['body'] != '']\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "top_df.to_csv('../Datasets/train_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we pre-process the test sets in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kaggle set\n",
    "kaggle_set =  pd.read_csv('../Datasets/Kaggle/test.csv', encoding='cp1252')\n",
    "kaggle_set.columns = ['id', 'body']\n",
    "kaggle_set = prep_data(kaggle_set)\n",
    "\n",
    "# Clean test sets\n",
    "test_df['body'] = test_df['body'].apply(lambda x: ' '.join([word for word in x.split() if word in top_words]))\n",
    "kaggle_set['body'] = kaggle_set['body'].apply(lambda x: ' '.join([word for word in x.split() if word in top_words])if len(x.split()) > 5 else x)\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "test_df.to_csv('../Datasets/test_cleaned.csv', index=False)\n",
    "kaggle_set.to_csv('../Datasets/Kaggle/kaggle_test_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
