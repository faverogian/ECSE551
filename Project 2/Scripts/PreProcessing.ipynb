{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Suppress output of following line and do not output True or False\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the training data from the given .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('../Datasets/train.csv', encoding='cp1252')\n",
    "df.columns = ['body', 'subreddit']\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "df['body'] = df['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import train_test_split\\n\\n# Split dataset into training and testing\\nX_df = df['body']\\ny_df = df['subreddit']\\ny_df = y_df.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, stratify=y_df, random_state=42)\\n\\n# Package X_test and y_test into a dataframe\\ny_test = y_test.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\\ntest_df = pd.concat([X_test, y_test], axis=1)\\ntest_df.to_csv('../Datasets/test_w_labels.csv', index=False)\\n\\n# Package X_train and y_train into a dataframe\\ny_train = y_train.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\\ndf = pd.concat([X_train, y_train], axis=1)\\n\\n# Convert all characters to lowercase\\ndf['body'] = df['body'].str.lower()\""
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and testing\n",
    "X_df = df['body']\n",
    "y_df = df['subreddit']\n",
    "y_df = y_df.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, stratify=y_df, random_state=42)\n",
    "\n",
    "# Package X_test and y_test into a dataframe\n",
    "y_test = y_test.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df.to_csv('../Datasets/test_w_labels.csv', index=False)\n",
    "\n",
    "# Package X_train and y_train into a dataframe\n",
    "y_train = y_train.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "df['body'] = df['body'].str.lower()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before pre-processing the data, it can be helpful to identify the characters we are dealing with in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   character  frequency\n",
      "0                 59253\n",
      "1          e      34050\n",
      "2          t      24093\n",
      "3          a      21378\n",
      "4          o      20234\n",
      "..       ...        ...\n",
      "88         œ          2\n",
      "89         °          2\n",
      "90         ã          1\n",
      "91         á          1\n",
      "92         ë          1\n",
      "\n",
      "[93 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the frequency of each appearance of a character in the dataset\n",
    "def find_frequency(df):\n",
    "    frequency = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for character in row['body']:\n",
    "            if character in frequency:\n",
    "                frequency[character] += 1\n",
    "            else:\n",
    "                frequency[character] = 1\n",
    "    return frequency\n",
    "\n",
    "# Make function to create pandas dataframe of frequency of each character\n",
    "def make_frequency_df(frequency):\n",
    "    freq_df = pd.DataFrame.from_dict(frequency, orient='index', columns=['frequency'])\n",
    "    freq_df = freq_df.sort_values(by=['frequency'], ascending=False)\n",
    "    freq_df['character'] = freq_df.index\n",
    "    freq_df = freq_df.reset_index(drop=True)\n",
    "    freq_df = freq_df[['character', 'frequency']]\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "freq = find_frequency(df)\n",
    "freq_df = make_frequency_df(freq)\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is quite a distribution of characters here. We are going to try and keep as many as possible, but also try to align things like apostrophes that have different representations in different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align encodings\n",
    "df['body'] = df['body'].str.replace('“', '\"')\n",
    "df['body'] = df['body'].str.replace('”', '\"')\n",
    "df['body'] = df['body'].str.replace('’', \"'\")\n",
    "df['body'] = df['body'].str.replace('‘', \"'\")\n",
    "df['body'] = df['body'].str.replace('—', '-')\n",
    "df['body'] = df['body'].str.replace('–', '-')\n",
    "df['body'] = df['body'].str.replace('\\n', ' ')\n",
    "df['body'] = df['body'].str.replace('/', ' ')\n",
    "df['body'] = df['body'].str.replace('#x200b', ' ')\n",
    "df['body'] = df['body'].str.replace('-', ' ')\n",
    "\n",
    "# Remove basic punctuation\n",
    "translator = str.maketrans('', '', '<>\"°œ!\\()*+,.:;=?[\\\\]^_`{|}~1234567890')\n",
    "df['body'] = df['body'].str.translate(translator)\n",
    "\n",
    "# Replace accented characters with unaccented characters\n",
    "translator = str.maketrans('àáâãäåçèéêëìíîïñòóôõöùúûüýÿ', 'aaaaaaceeeeiiiinooooouuuuyy')\n",
    "df['body'] = df['body'].str.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preprocessing is helpful prior to tokenization. This includes lemmatization and removing stop-words (a, an, the) in both English and French, since we are dealing with cities like Toronto, Montreal, and Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with their lemmings\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_verbs(text):\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "\n",
    "def lemmatize_nouns(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_nouns(x.split()))\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_verbs(x))\n",
    "\n",
    "# Reconcatenate the words into a string\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "stop_words = set(stopwords.words('french'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and testing\n",
    "X_df = df['body']\n",
    "y_df = df['subreddit']\n",
    "y_df = y_df.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, stratify=y_df, random_state=42)\n",
    "\n",
    "# Package X_test and y_test into a dataframe\n",
    "y_test = y_test.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df.to_csv('../Datasets/test_w_labels.csv', index=False)\n",
    "\n",
    "# Package X_train and y_train into a dataframe\n",
    "y_train = y_train.map({0: 'Toronto', 1: 'London', 2: 'Paris', 3: 'Montreal'})\n",
    "df = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can do an in-depth analysis on the most common words shared between each subreddit and eliminate them from the dataset to improve class distinguishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_freq(subreddit, df):\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    sub_word_index = {}\n",
    "    \n",
    "    for row in subreddit_df['body']:\n",
    "        for word in row.split():\n",
    "            if word in sub_word_index:\n",
    "                sub_word_index[word] += 1\n",
    "            else:\n",
    "                sub_word_index[word] = 1\n",
    "\n",
    "    # Sort the dictionary by value\n",
    "    sub_word_index = dict(sorted(sub_word_index.items(), key=lambda item: item[1], reverse=True))\n",
    "    return list(sub_word_index.items())\n",
    "    \n",
    "def remove_uncommon_words(subreddit):\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    \n",
    "    # Build a vocabulary of words and how many samples they appear in\n",
    "    vocab = {}\n",
    "    for row in subreddit_df['body']:\n",
    "        for word in row.split():\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "\n",
    "    # Remove words that appear in few samples\n",
    "    for word in list(vocab):\n",
    "        if vocab[word] < 0.01 * len(subreddit_df):\n",
    "            del vocab[word]\n",
    "\n",
    "    # Remove all words that are not in the vocabulary\n",
    "    subreddit_df['body'] = subreddit_df['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "    return subreddit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first conduct an analysis to examine the most important words for each subreddit. Words that appear in less than 1% of the samples are removed. This reduces the total vocabulary in the dataset by about half. Next, the top 250 words that are most common among every subreddit are removed as they muddy the waters when it comes to classification. By stripping these words, we make the classes more distinguishable from one another as they have less in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "\n",
    "toronto_df = remove_uncommon_words('Toronto')\n",
    "montreal_df = remove_uncommon_words('Montreal')\n",
    "paris_df = remove_uncommon_words('Paris')\n",
    "london_df = remove_uncommon_words('London')\n",
    "\n",
    "new_df = pd.concat([toronto_df, london_df, paris_df, montreal_df])\n",
    "\n",
    "# Build vocabulary of words in new_df\n",
    "vocab = []\n",
    "for row in new_df['body']:\n",
    "    for word in row.split():\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "# Get the frequency of each word in the vocabulary (how many samples it appears in)\n",
    "subdict = {}\n",
    "for subreddit in subreddits:\n",
    "    subdict[subreddit] = get_highest_freq(subreddit, new_df)\n",
    "for key in subdict:\n",
    "    subdict[key] = [item[0] for item in subdict[key]]\n",
    "\n",
    "# Get aggregate index of words in the vocabulary\n",
    "agg_index = {}\n",
    "for word in vocab:\n",
    "    agg_index[word] = 0\n",
    "    for key in subdict:\n",
    "        if word in subdict[key]:\n",
    "            agg_index[word] += subdict[key].index(word)\n",
    "        else:\n",
    "            agg_index[word] += len(subdict[key])\n",
    "\n",
    "# Sort the dictionary by value\n",
    "agg_index = dict(sorted(agg_index.items(), key=lambda item: item[1], reverse=False))\n",
    "\n",
    "# Remove the most common words from new_df\n",
    "for word in list(agg_index)[:300]:\n",
    "    for sample in new_df['body']:\n",
    "        if word in sample.split():\n",
    "            new_df['body'] = new_df['body'].replace(sample, sample.replace(word + ' ', ''))\n",
    "\n",
    "# Remove words not in new_df vocab from df\n",
    "for sample in df['body']:\n",
    "    for word in sample.split():\n",
    "        if word not in vocab:\n",
    "            df['body'] = df['body'].replace(sample, sample.replace(word + ' ', ''))\n",
    "\n",
    "# Remove most common words from df\n",
    "for word in list(agg_index)[:300]:\n",
    "    for sample in df['body']:\n",
    "        if word in sample.split():\n",
    "            df['body'] = df['body'].replace(sample, sample.replace(word + ' ', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency of each word in the vocabulary (how many samples it appears in)\n",
    "subdict = {}\n",
    "for subreddit in subreddits:\n",
    "    subdict[subreddit] = get_highest_freq(subreddit, new_df)\n",
    "for key in subdict:\n",
    "    subdict[key] = [item[0] for item in subdict[key]]\n",
    "\n",
    "'''# Find words in subdict that appear in every subreddit\n",
    "common_words = []\n",
    "for word in subdict[subreddits[0]]:\n",
    "    if word in subdict[subreddits[1]] and word in subdict[subreddits[2]] and word in subdict[subreddits[3]]:\n",
    "        common_words.append(word)\n",
    "\n",
    "# Get words that appear most frequently in Montreal and Toronto to reduce overlap\n",
    "for word in subdict[subreddits[3]][:150]:\n",
    "    if word in subdict[subreddits[0]][:150]:\n",
    "        common_words.append(word)\n",
    "\n",
    "# Get words that appear most frequently in Montreal and Paris to reduce overlap\n",
    "for word in subdict[subreddits[3]][:150]:\n",
    "    if word in subdict[subreddits[2]][:150]:\n",
    "        common_words.append(word)\n",
    "\n",
    "# Remove extra words from df\n",
    "for word in common_words:\n",
    "    for sample in df['body']:\n",
    "        if word in sample.split():\n",
    "            df['body'] = df['body'].replace(sample, sample.replace(word + ' ', ''))'''\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "df.to_csv('../Datasets/train_prep.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
