{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Suppress output of following line and do not output True or False\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the training data from the given .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset and remove non-utf-8 characters\n",
    "df = pd.read_csv('../Datasets/test.csv', encoding='cp1252')\n",
    "df.columns = ['id', 'body']\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "df['body'] = df['body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before pre-processing the data, it can be helpful to identify the characters we are dealing with in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   character  frequency\n",
      "0                 22179\n",
      "1          e      12796\n",
      "2          t       8807\n",
      "3          a       8030\n",
      "4          o       7568\n",
      "..       ...        ...\n",
      "84         ï          3\n",
      "85         ~          2\n",
      "86         ü          2\n",
      "87         @          1\n",
      "88         ã          1\n",
      "\n",
      "[89 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the frequency of each appearance of a character in the dataset\n",
    "def find_frequency(df):\n",
    "    frequency = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for character in row['body']:\n",
    "            if character in frequency:\n",
    "                frequency[character] += 1\n",
    "            else:\n",
    "                frequency[character] = 1\n",
    "    return frequency\n",
    "\n",
    "# Make function to create pandas dataframe of frequency of each character\n",
    "def make_frequency_df(frequency):\n",
    "    freq_df = pd.DataFrame.from_dict(frequency, orient='index', columns=['frequency'])\n",
    "    freq_df = freq_df.sort_values(by=['frequency'], ascending=False)\n",
    "    freq_df['character'] = freq_df.index\n",
    "    freq_df = freq_df.reset_index(drop=True)\n",
    "    freq_df = freq_df[['character', 'frequency']]\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "freq = find_frequency(df)\n",
    "freq_df = make_frequency_df(freq)\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is quite a distribution of characters here. We are going to try and keep as many as possible, but also try to align things like apostrophes that have different representations in different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align encodings\n",
    "df['body'] = df['body'].str.replace('“', '\"')\n",
    "df['body'] = df['body'].str.replace('”', '\"')\n",
    "df['body'] = df['body'].str.replace('’', \"'\")\n",
    "df['body'] = df['body'].str.replace('‘', \"'\")\n",
    "df['body'] = df['body'].str.replace('—', '-')\n",
    "df['body'] = df['body'].str.replace('–', '-')\n",
    "df['body'] = df['body'].str.replace('\\n', ' ')\n",
    "df['body'] = df['body'].str.replace('/', ' ')\n",
    "df['body'] = df['body'].str.replace('#x200b', ' ')\n",
    "df['body'] = df['body'].str.replace('-', ' ')\n",
    "\n",
    "# Remove basic punctuation\n",
    "translator = str.maketrans('', '', '<>\"°œ!\\()*+,.:;=?[\\\\]^_`{|}~1234567890')\n",
    "df['body'] = df['body'].str.translate(translator)\n",
    "\n",
    "# Replace accented characters with unaccented characters\n",
    "translator = str.maketrans('àáâãäåçèéêëìíîïñòóôõöùúûüýÿ', 'aaaaaaceeeeiiiinooooouuuuyy')\n",
    "df['body'] = df['body'].str.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-examine the frequency of each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   character  frequency\n",
      "0                 23538\n",
      "1          e      13390\n",
      "2          t       8807\n",
      "3          a       8175\n",
      "4          o       7588\n",
      "5          i       7207\n",
      "6          n       7192\n",
      "7          s       7141\n",
      "8          r       6050\n",
      "9          l       4355\n",
      "10         u       4052\n",
      "11         h       3567\n",
      "12         d       3558\n",
      "13         c       3220\n",
      "14         m       2621\n",
      "15         p       2560\n",
      "16         g       1851\n",
      "17         f       1756\n",
      "18         y       1742\n",
      "19         b       1487\n",
      "20         w       1410\n",
      "21         v       1268\n",
      "22         '        858\n",
      "23         k        716\n",
      "24         q        456\n",
      "25         j        396\n",
      "26         x        260\n",
      "27         z         86\n",
      "28         $         20\n",
      "29                   20\n",
      "30         %         17\n",
      "31         …         11\n",
      "32         &         11\n",
      "33         «         10\n",
      "34         »         10\n",
      "35         £          6\n",
      "36         €          6\n",
      "37         #          3\n",
      "38         @          1\n"
     ]
    }
   ],
   "source": [
    "freq_aligned = find_frequency(df)\n",
    "freq_df_aligned = make_frequency_df(freq_aligned)\n",
    "print(freq_df_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preprocessing is helpful prior to tokenization. This includes lemmatization and removing stop-words (a, an, the) in both English and French, since we are dealing with cities like Toronto, Montreal, and Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with their lemmings\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_verbs(text):\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "\n",
    "def lemmatize_nouns(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_nouns(x.split()))\n",
    "df['body'] = df['body'].apply(lambda x: lemmatize_verbs(x))\n",
    "\n",
    "# Reconcatenate the words into a string\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "stop_words = set(stopwords.words('french'))\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transfer the knowledge of common words from the training set to further process the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab.txt into a list\n",
    "with open('vocab.txt', 'r') as file:\n",
    "    vocab = file.read().splitlines()\n",
    "\n",
    "# Load agg_index.txt into a list\n",
    "with open('agg_index.txt', 'r') as file:\n",
    "    agg_index = file.read().splitlines()\n",
    "\n",
    "# Remove words not in vocab from df\n",
    "for sample in df['body']:\n",
    "    for word in sample.split():\n",
    "        if word not in vocab:\n",
    "            df['body'] = df['body'].replace(sample, sample.replace(word + ' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               body\n",
      "0   0  even people uber address toronto people live p...\n",
      "1   1  undoubtedly commuter electric transit rather p...\n",
      "2   2  shopper use decent sale loblaws remember every...\n",
      "3   3  yeah anti immigration talk lead something like...\n",
      "4   4  talk female assistant help nygard lure woman b...\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaned test set is saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df.to_csv('../Datasets/test_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
