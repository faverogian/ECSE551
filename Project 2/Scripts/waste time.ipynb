{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that is to be done is to import the data and generate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Datasets/train_cleaned.csv')\n",
    "\n",
    "# Split dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['body']\n",
    "y = df['subreddit']\n",
    "y = y.map({'Toronto': 0, 'London': 1, 'Montreal': 2, 'Paris': 3})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define some pre-made models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "multi_nb = MultinomialNB()\n",
    "random_forest = RandomForestClassifier()\n",
    "log_reg = LogisticRegression()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "models = [multi_nb, random_forest, log_reg, svc, knn]\n",
    "\n",
    "def evaluate_models(models):\n",
    "    for model in models:\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        print(model, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first try CountVectorizer encoding of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() 0.6944444444444444\n",
      "RandomForestClassifier() 0.5944444444444444\n",
      "LogisticRegression() 0.6555555555555556\n",
      "SVC() 0.4722222222222222\n",
      "KNeighborsClassifier() 0.29444444444444445\n"
     ]
    }
   ],
   "source": [
    "# Vectorize training and testing data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "x_train = vectorizer.fit_transform(X_train)\n",
    "x_test = vectorizer.transform(X_test)\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try a Tfidf encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() 0.7\n",
      "RandomForestClassifier() 0.5444444444444444\n",
      "LogisticRegression() 0.6777777777777778\n",
      "SVC() 0.6333333333333333\n",
      "KNeighborsClassifier() 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Vectorize training and testing data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "x_train = vectorizer.fit_transform(X_train)\n",
    "x_test = vectorizer.transform(X_test)\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also import the tokenizer models we previously trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, encoding_func, arguments={}):\n",
    "        self.encoding_func = encoding_func\n",
    "        self.arguments = arguments\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self.encoding_func(text, **self.arguments)\n",
    "        if isinstance(tokens, list):\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens.tokens\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file(\"Tokenizers/bpe_tokenizer.json\")\n",
    "sp_tokenizer = spm.SentencePieceProcessor(model_file=\"Tokenizers/sp_model.model\")\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Load them all into a list\n",
    "tokenizer_models = [\n",
    "    TokenizerWrapper(bpe_tokenizer.encode),\n",
    "    TokenizerWrapper(sp_tokenizer.encode, {'out_type': str}),\n",
    "    TokenizerWrapper(generate_ngrams, {'n': 2}),\n",
    "    TokenizerWrapper(generate_ngrams, {'n': 3}),\n",
    "    TokenizerWrapper(whitespace_tokenizer)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingTransformer:\n",
    "    def __init__(self, transform_method):\n",
    "        self.transform_method = transform_method\n",
    "\n",
    "    def transform(self, X):\n",
    "        try:\n",
    "            return self.transform_method[X]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "word_vec = Word2Vec.load('Embeddings/word2vec_model.bin')\n",
    "fast_vec = FastText.load('Embeddings/fasttext_model.bin')\n",
    "\n",
    "# Load them all into a list\n",
    "embedding_models = [\n",
    "    EmbeddingTransformer(word_vec.wv),\n",
    "    EmbeddingTransformer(fast_vec.wv)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start analyzing the performance of various tokenizations, embeddings, and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(tokenizer_model, embedding_model, corpus):\n",
    "    sample_embeddings = []\n",
    "    tokenized_samples = [tokenizer_model.tokenize(text) for text in corpus]\n",
    "    for sample in tokenized_samples:\n",
    "        embeddings = []\n",
    "        for word in sample:\n",
    "            try:\n",
    "                embeddings.append(embedding_model.transform(word))\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        embeddings = [x for x in embeddings if x is not None]\n",
    "        sample_embeddings.append(np.mean(embeddings, axis=0))\n",
    "\n",
    "    return np.array(sample_embeddings)\n",
    "\n",
    "def get_scores(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_train, y_train), model.score(X_test, y_test)\n",
    "\n",
    "# Try and train one model\n",
    "x_train = generate_embeddings(tokenizer_models[0], embedding_models[0], X_train.to_list())\n",
    "x_test = generate_embeddings(tokenizer_models[0], embedding_models[0], X_test.to_list())\n",
    "\n",
    "print(get_scores(models[3], x_train, x_test, y_train, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
