{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, pre_tokenizers, models\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded and converted into a list of strings to be fed into the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert to a list of strings\n",
    "df = pd.read_csv('../Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()  # Assuming 'body' is the column containing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tokenizer is used to convert the body text into tokens before feeding them into the embedding model for training. We load in the two previously trained tokenizers here as well as create two simple ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pre-trained BPE tokenizer\n",
    "bpe_tokenizer = Tokenizer.from_file(\"../Tokenizers/bpe_tokenizer.json\")\n",
    "\n",
    "# Load in the pre-trained SentencePiece tokenizer\n",
    "sp_tokenizer = spm.SentencePieceProcessor(model_file=\"../Tokenizers/sp_model.model\")\n",
    "\n",
    "# Create a n-gram tokenizer\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Create a whitespace tokenizer\n",
    "def whitespace_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use each tokenizer and train a Word2Vec, FastText, and binary embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Embeddings/count_model.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tokens of the data in many different forms\n",
    "whitespace_tokenized_corpus = [whitespace_tokenizer(text) for text in corpus]\n",
    "n2gram_tokenized_corpus = [generate_ngrams(text, 2) for text in corpus]\n",
    "n3gram_tokenized_corpus = [generate_ngrams(text, 3) for text in corpus]\n",
    "bpe_tokenized_corpus = [bpe_tokenizer.encode(text).tokens for text in corpus]\n",
    "sp_tokenized_corpus = [sp_tokenizer.encode(text, out_type=str) for text in corpus]\n",
    "\n",
    "# Combine all the list of lists\n",
    "tokenized_corpus = whitespace_tokenized_corpus + n2gram_tokenized_corpus + n3gram_tokenized_corpus + bpe_tokenized_corpus + sp_tokenized_corpus\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=100, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=0, \n",
    "                 negative=5, \n",
    "                 epochs=10,\n",
    "                 sample=1e-5)\n",
    "model.save(\"../Embeddings/word2vec_model.bin\")\n",
    "\n",
    "# Train a FastText model\n",
    "model = FastText(sentences=tokenized_corpus, \n",
    "                 vector_size=100, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=0, \n",
    "                 negative=5, \n",
    "                 epochs=10,\n",
    "                 sample=1e-5)\n",
    "model.save(\"../Embeddings/fasttext_model.bin\")\n",
    "\n",
    "model = TfidfVectorizer(max_features=3000)\n",
    "model.fit([item for sublist in tokenized_corpus for item in sublist])\n",
    "joblib.dump(model, \"../Embeddings/tfidf_model.joblib\")\n",
    "\n",
    "# Train a Count Vectorizer model\n",
    "model = CountVectorizer(max_features=3000)\n",
    "model.fit([item for sublist in tokenized_corpus for item in sublist])\n",
    "joblib.dump(model, \"../Embeddings/count_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the tokenize-embedding strategy using a given pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.003420533, 0.047891144, 0.042080585, 0.009...</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0022828227, 0.04957727, 0.0435968, 0.01011...</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.0026055586, 0.043765545, 0.036465846, 0.00...</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0011125315, 0.04322434, 0.037288498, 0.008...</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00599845, 0.057221085, 0.049849674, 0.0111...</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body subreddit\n",
       "0  [-0.003420533, 0.047891144, 0.042080585, 0.009...   Toronto\n",
       "1  [-0.0022828227, 0.04957727, 0.0435968, 0.01011...   Toronto\n",
       "2  [-0.0026055586, 0.043765545, 0.036465846, 0.00...   Toronto\n",
       "3  [-0.0011125315, 0.04322434, 0.037288498, 0.008...   Toronto\n",
       "4  [-0.00599845, 0.057221085, 0.049849674, 0.0111...   Toronto"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make new dataset that has all body text replaced with their embedding using the BPE tokenizer and Word2Vec model\n",
    "df = pd.read_csv('../Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file(\"../Tokenizers/bpe_tokenizer.json\")\n",
    "tokenized_corpus = [bpe_tokenizer.encode(text).tokens for text in corpus]\n",
    "\n",
    "model = Word2Vec.load(\"../Embeddings/word2vec_model.bin\")\n",
    "word2vec_embeddings = []\n",
    "\n",
    "# Every sample is one embedding that is the average of all the word embeddings in the sample\n",
    "for sample in tokenized_corpus:\n",
    "    embeddings = []\n",
    "    for word in sample:\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    word2vec_embeddings.append(np.mean(embeddings, axis=0))\n",
    "\n",
    "# Replace the body text in df with the embeddings\n",
    "df['body'] = word2vec_embeddings\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# See how many times \"This\" appears in the tokenized corpus\n",
    "count = 0\n",
    "for sample in tokenized_corpus:\n",
    "    if \"sample\" in sample:\n",
    "        count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
