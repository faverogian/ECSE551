{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, pre_tokenizers, models\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded and converted into a list of strings to be fed into the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert to a list of strings\n",
    "df = pd.read_csv('../Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()  # Assuming 'body' is the column containing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tokenizer is used to convert the body text into tokens before feeding them into the embedding model for training. We load in the two previously trained tokenizers here as well as create two simple ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pre-trained BPE tokenizer\n",
    "bpe_tokenizer = Tokenizer.from_file(\"../Tokenizers/bpe_tokenizer.json\")\n",
    "\n",
    "# Load in the pre-trained SentencePiece tokenizer\n",
    "sp_tokenizer = spm.SentencePieceProcessor(model_file=\"../Tokenizers/sp_model.model\")\n",
    "\n",
    "# Create a n-gram tokenizer\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Create a whitespace tokenizer\n",
    "def whitespace_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use each tokenizer and train a Word2Vec, FastText, and binary embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Embeddings/count_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tokens of the data in many different forms\n",
    "whitespace_tokenized_corpus = [whitespace_tokenizer(text) for text in corpus]\n",
    "n2gram_tokenized_corpus = [generate_ngrams(text, 2) for text in corpus]\n",
    "n3gram_tokenized_corpus = [generate_ngrams(text, 3) for text in corpus]\n",
    "bpe_tokenized_corpus = [bpe_tokenizer.encode(text).tokens for text in corpus]\n",
    "sp_tokenized_corpus = [sp_tokenizer.encode(text, out_type=str) for text in corpus]\n",
    "\n",
    "# Combine all the list of lists\n",
    "tokenized_corpus = whitespace_tokenized_corpus + n2gram_tokenized_corpus + n3gram_tokenized_corpus + bpe_tokenized_corpus + sp_tokenized_corpus\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=100, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=0, \n",
    "                 negative=5, \n",
    "                 epochs=10, \n",
    "                 sample=1e-5)\n",
    "model.save(\"../Embeddings/word2vec_model.bin\")\n",
    "\n",
    "# Train a FastText model\n",
    "model = FastText(sentences=tokenized_corpus, \n",
    "                 vector_size=100, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=0, \n",
    "                 negative=5, \n",
    "                 epochs=10, \n",
    "                 sample=1e-5)\n",
    "model.save(\"../Embeddings/fasttext_model.bin\")\n",
    "\n",
    "# Train a TFid model\n",
    "model = TfidfVectorizer(max_features=3000)\n",
    "model.fit(corpus)\n",
    "joblib.dump(model, \"../Embeddings/tfidf_model.joblib\")\n",
    "\n",
    "# Train a Count Vectorizer model\n",
    "model = CountVectorizer(max_features=3000)\n",
    "model.fit(corpus)\n",
    "joblib.dump(model, \"../Embeddings/count_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
