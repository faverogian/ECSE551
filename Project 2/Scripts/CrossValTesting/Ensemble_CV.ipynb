{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that is to be done is to import the data and generate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from prep import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../../Datasets/train.csv', encoding='cp1252')\n",
    "\n",
    "processing = 'word_replacement'\n",
    "\n",
    "if processing in ['basic', 'common_words', 'mutual', 'word_replacement']:\n",
    "    df = prep_data(df)\n",
    "\n",
    "# Split data using KFold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(max_iter=1000, C=0.1)),\n",
    "    ('svm', SVC(C=2, degree=2)),\n",
    "    ('rfc', OneVsRestClassifier(RandomForestClassifier(max_depth=30, n_estimators=100, random_state=42)))\n",
    "]\n",
    "\n",
    "final_estimator = LogisticRegression(max_iter=1000, C=0.1)\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', ensemble)\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer__ngram_range': [(1, 1)],  # Consider different n-gram ranges\n",
    "}\n",
    "# Make list to store accuracies\n",
    "test_acc = []\n",
    "y_preds = []\n",
    "y_vals = []\n",
    "\n",
    "# Iterate through folds\n",
    "for train_index, val_index in kf.split(df['body'], df['subreddit']):\n",
    "    # Split data\n",
    "    train = df.iloc[train_index]\n",
    "    val = df.iloc[val_index]\n",
    "    vocab = build_vocab(train)\n",
    "\n",
    "    if processing in ['word_replacement']:\n",
    "        val = word_replacement(val)\n",
    "        train = word_replacement(train)\n",
    "\n",
    "    # Reduce features based on mutual information\n",
    "    subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "\n",
    "    if processing in ['common_words', 'mutual', 'word_replacement']:\n",
    "        train, vocab = remove_common_words(train, subreddits, 300)\n",
    "\n",
    "    if processing in ['mutual', 'word_replacement']:\n",
    "        train = mutual_info_transform(train, 3500)\n",
    "        train, vocab = remove_common_words(train, subreddits, 25)\n",
    "\n",
    "    # Remove words not in vocab from val\n",
    "    val['body'] = val['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = train['body']\n",
    "    y_train = train['subreddit']\n",
    "    y_train = y_train.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "    X_val = val['body']\n",
    "    y_val = val['subreddit']\n",
    "    y_val = y_val.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_vals.extend(y_val)\n",
    "    y_preds.extend(grid_search.predict(X_val))\n",
    "    test_acc.append(grid_search.score(X_val, y_val))\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Accuracy: {test_acc[-1]}')\n",
    "\n",
    "test_acc = np.mean(test_acc)\n",
    "print(f'Average accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import plot_conf_mat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plot overall confusion matrix with proper city names\n",
    "conf_mat = confusion_matrix(y_vals, y_preds)\n",
    "plot_conf_mat(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on full dataset and test on Kaggle test set\n",
    "kaggle_test = pd.read_csv('../../Datasets/Kaggle/test.csv', encoding='cp1252')\n",
    "test_body = kaggle_test['body'].copy()\n",
    "kaggle_test = prep_data(kaggle_test)\n",
    "kaggle_test = word_replacement(kaggle_test)\n",
    "\n",
    "test_df = df.copy()\n",
    "\n",
    "# Reduce features based on mutual information\n",
    "subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "test_df = word_replacement(test_df)\n",
    "test_df, _ = remove_common_words(test_df, subreddits, 300)\n",
    "test_df = mutual_info_transform(test_df, 3500)\n",
    "test_df, vocab = remove_common_words(test_df, subreddits, 25)\n",
    "\n",
    "# Remove words not in vocab from kaggle test set\n",
    "kaggle_test['body'] = kaggle_test['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Split into X and y\n",
    "X_train = test_df['body']\n",
    "y_train = test_df['subreddit']\n",
    "\n",
    "# Fit on full training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "X_test = kaggle_test['body']\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Make submission file\n",
    "submission = generate_kaggle_submission(kaggle_test, y_pred)\n",
    "submission.to_csv('../../Datasets/Kaggle/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
