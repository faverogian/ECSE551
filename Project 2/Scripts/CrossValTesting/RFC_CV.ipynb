{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that is to be done is to import the data and generate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "{'classifier__estimator__max_depth': 30, 'classifier__estimator__n_estimators': 100, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 1)}\n",
      "Accuracy: 0.6736111111111112\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "{'classifier__estimator__max_depth': 50, 'classifier__estimator__n_estimators': 50, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 1)}\n",
      "Accuracy: 0.6944444444444444\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "{'classifier__estimator__max_depth': 50, 'classifier__estimator__n_estimators': 100, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 1)}\n",
      "Accuracy: 0.7083333333333334\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "{'classifier__estimator__max_depth': 30, 'classifier__estimator__n_estimators': 100, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 1)}\n",
      "Accuracy: 0.6527777777777778\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "{'classifier__estimator__max_depth': 50, 'classifier__estimator__n_estimators': 100, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 2)}\n",
      "Accuracy: 0.7342657342657343\n",
      "Average accuracy: 0.6926864801864802\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from prep import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../../Datasets/train.csv', encoding='cp1252')\n",
    "\n",
    "# Do some basic cleaning\n",
    "df = prep_data(df)\n",
    "\n",
    "# Split data using KFold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=3000)),\n",
    "    ('classifier', OneVsRestClassifier(RandomForestClassifier(random_state=42))) \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)], \n",
    "    'classifier__estimator__n_estimators': [50, 100],  \n",
    "    'classifier__estimator__max_depth': [20, 30, 50],\n",
    "}\n",
    "# Make list to store accuracies\n",
    "test_acc = []\n",
    "\n",
    "# Iterate through folds\n",
    "for train_index, val_index in kf.split(df['body'], df['subreddit']):\n",
    "    # Split data\n",
    "    train = df.iloc[train_index]\n",
    "    val = df.iloc[val_index]\n",
    "\n",
    "    # Reduce features based on mutual information\n",
    "    subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "    train, _ = remove_common_words(train, subreddits, 300)\n",
    "    train = mutual_info_transform(train, 3250)\n",
    "    train, vocab = remove_common_words(train, subreddits, 25)\n",
    "\n",
    "    # Remove words not in vocab from val\n",
    "    val['body'] = val['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = train['body']\n",
    "    y_train = train['subreddit']\n",
    "    y_train = y_train.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "    X_val = val['body']\n",
    "    y_val = val['subreddit']\n",
    "    y_val = y_val.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    test_acc.append(grid_search.score(X_val, y_val))\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Accuracy: {test_acc[-1]}')\n",
    "\n",
    "test_acc = np.mean(test_acc)\n",
    "print(f'Average accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on full dataset and test on Kaggle test set\n",
    "kaggle_test = pd.read_csv('../../Datasets/Kaggle/test.csv', encoding='cp1252')\n",
    "kaggle_test = prep_data(kaggle_test)\n",
    "test_body = kaggle_test['body'].copy()\n",
    "\n",
    "test_df = df.copy()\n",
    "\n",
    "# Reduce features based on mutual information\n",
    "subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "test_df, _ = remove_common_words(test_df, subreddits, 300)\n",
    "test_df = mutual_info_transform(test_df, 3250)\n",
    "test_df, vocab = remove_common_words(test_df, subreddits, 25)\n",
    "\n",
    "# Remove words not in vocab from kaggle test set\n",
    "kaggle_test['body'] = kaggle_test['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Split into X and y\n",
    "X_train = test_df['body']\n",
    "y_train = test_df['subreddit']\n",
    "\n",
    "# Fit on full training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "X_test = kaggle_test['body']\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Make submission file\n",
    "submission = generate_kaggle_submission(kaggle_test, y_pred)\n",
    "\n",
    "# Compare result to the best results\n",
    "best_result = pd.read_csv('../../Datasets/Kaggle/best_score.csv')\n",
    "\n",
    "# Make dataframe that has the body and the two predictions\n",
    "compare = pd.DataFrame({'body': test_body, 'y_pred': y_pred, 'y_best': best_result['subreddit']})\n",
    "\n",
    "# Remove the rows where the two predictions are the same\n",
    "compare = compare[compare['y_pred'] != compare['y_best']]\n",
    "\n",
    "# Output the dataframe to a csv file\n",
    "compare.to_csv('compare.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
