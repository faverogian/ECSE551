{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that is to be done is to import the data and generate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + tf.exp(-z))\n",
    "\n",
    "def classifier_model(layer_size, num_layers, activation, input_dim, dropout_rate=0.7):\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # Add first layer\n",
    "    network.add(layers.Dense(layer_size, \n",
    "                             input_dim=input_dim, \n",
    "                             activation=activation, \n",
    "                             kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                             kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        network.add(layers.Dense(layer_size, \n",
    "                                 activation=activation, \n",
    "                                 kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "        network.add(BatchNormalization())\n",
    "        network.add(layers.Dropout(dropout_rate))\n",
    "    network.add(layers.Dense(4, activation='softmax')) # Add the output layer\n",
    "\n",
    "    # Compile the network\n",
    "    network.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return network\n",
    "\n",
    "model_1 = classifier_model(\n",
    "    layer_size=300,\n",
    "    num_layers=5, \n",
    "    input_dim=3000,\n",
    "    activation=sigmoid,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 23ms/step - loss: 2.8985 - accuracy: 0.2626\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.8639 - accuracy: 0.4591\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.5229 - accuracy: 0.6191\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.1833 - accuracy: 0.7409\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.7190 - accuracy: 0.8765\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.4893 - accuracy: 0.9461\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.4094 - accuracy: 0.9478\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.3653 - accuracy: 0.9548\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.2759 - accuracy: 0.9635\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.1753 - accuracy: 0.9896\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.4825 - accuracy: 0.2500\n",
      "Accuracy: 0.25\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3525 - accuracy: 0.4261\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.2942 - accuracy: 0.6452\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.6345 - accuracy: 0.8417\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4272 - accuracy: 0.9061\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3036 - accuracy: 0.9478\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1507 - accuracy: 0.9757\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.1281 - accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.0470 - accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.9964 - accuracy: 0.9826\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.9191 - accuracy: 0.9878\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.2319 - accuracy: 0.2500\n",
      "Accuracy: 0.25\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.8663 - accuracy: 0.5078\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.5103 - accuracy: 0.7983\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.1803 - accuracy: 0.9148\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.0605 - accuracy: 0.9443\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.9698 - accuracy: 0.9704\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.9339 - accuracy: 0.9704\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.8694 - accuracy: 0.9826\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.7879 - accuracy: 0.9861\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.7540 - accuracy: 0.9809\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.7036 - accuracy: 0.9896\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.0410 - accuracy: 0.2500\n",
      "Accuracy: 0.25\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8586 - accuracy: 0.4852\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.4060 - accuracy: 0.7739\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.9707 - accuracy: 0.9304\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.8386 - accuracy: 0.9565\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.7811 - accuracy: 0.9670\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.7277 - accuracy: 0.9757\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6599 - accuracy: 0.9930\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6195 - accuracy: 0.9913\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6163 - accuracy: 0.9843\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5376 - accuracy: 0.9948\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8835 - accuracy: 0.2500\n",
      "Accuracy: 0.25\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.7794 - accuracy: 0.4479\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.2040 - accuracy: 0.7917\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.9360 - accuracy: 0.8750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.7160 - accuracy: 0.9583\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6757 - accuracy: 0.9653\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6169 - accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5696 - accuracy: 0.9878\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5601 - accuracy: 0.9792\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5507 - accuracy: 0.9861\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5068 - accuracy: 0.9792\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.8342 - accuracy: 0.2517\n",
      "Accuracy: 0.251748263835907\n",
      "Average accuracy: 0.2503496527671814\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from prep import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../../Datasets/train.csv', encoding='cp1252')\n",
    "\n",
    "# Do some basic cleaning\n",
    "df = prep_data(df)\n",
    "\n",
    "# Split data using KFold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Make list to store accuracies\n",
    "test_acc = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # Iterate through folds\n",
    "    for train_index, val_index in kf.split(df['body'], df['subreddit']):\n",
    "        # Split data\n",
    "        train = df.iloc[train_index]\n",
    "        val = df.iloc[val_index]\n",
    "\n",
    "        # Reduce features based on mutual information\n",
    "        subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "        train, _ = remove_common_words(train, subreddits, 300)\n",
    "        train = mutual_info_transform(train, 3250)\n",
    "        train, vocab = remove_common_words(train, subreddits, 25)\n",
    "\n",
    "        # Remove words not in vocab from val\n",
    "        val['body'] = val['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "        # Split into X and y\n",
    "        X_train = train['body']\n",
    "        y_train = train['subreddit']\n",
    "        y_train = y_train.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "        y_train = to_categorical(y_train)\n",
    "        X_val = val['body']\n",
    "        y_val = val['subreddit']\n",
    "        y_val = y_val.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "        y_val = to_categorical(y_val)\n",
    "\n",
    "        # Vectorize data\n",
    "        vectorizer = TfidfVectorizer(max_features=3000)\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_val = vectorizer.transform(X_val)\n",
    "\n",
    "        # Train model\n",
    "        X_train = X_train.toarray()\n",
    "        model_1.fit(X_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        X_val = X_val.toarray()\n",
    "        _, acc = model_1.evaluate(X_val, y_val)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "        # Print accuracy\n",
    "        print(f'Accuracy: {test_acc[-1]}')\n",
    "\n",
    "    test_acc = np.mean(test_acc)\n",
    "    print(f'Average accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.2616 - accuracy: 0.8081\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.5915 - accuracy: 0.7510\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.3374 - accuracy: 0.8122\n",
      "23/23 [==============================] - 0s 7ms/step\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Retrain on full dataset and test on Kaggle test set\n",
    "kaggle_test = pd.read_csv('../../Datasets/Kaggle/test.csv', encoding='cp1252')\n",
    "test_body = kaggle_test['body'].copy()\n",
    "kaggle_test = prep_data(kaggle_test)\n",
    "# kaggle_test = word_replacement(kaggle_test)\n",
    "\n",
    "test_df = df.copy()\n",
    "\n",
    "# Reduce features based on mutual information\n",
    "subreddits = ['Toronto', 'London', 'Paris', 'Montreal']\n",
    "# test_df = word_replacement(test_df)\n",
    "# test_df, _ = remove_common_words(test_df, subreddits, 300)\n",
    "# test_df = mutual_info_transform(test_df, 3250)\n",
    "# test_df, vocab = remove_common_words(test_df, subreddits, 25)\n",
    "\n",
    "# Remove words not in vocab from kaggle test set\n",
    "kaggle_test['body'] = kaggle_test['body'].apply(lambda x: ' '.join([word for word in x.split() if word in vocab]))\n",
    "\n",
    "# Split into X and y\n",
    "X_train = test_df['body']\n",
    "y_train = test_df['subreddit']\n",
    "y_train = y_train.map({'Toronto': 0, 'London': 1, 'Paris': 2, 'Montreal': 3})\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train)\n",
    "\n",
    "# Vectorize data\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(kaggle_test['body'])\n",
    "\n",
    "# Train model\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "model_1.fit(X_train, y_train, epochs=3, batch_size=16)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model_1.predict(X_train)\n",
    "\n",
    "# Make kaggle test answer array\n",
    "# First 70 are 0, next 70 are 1, etc.\n",
    "kaggle_ans = np.zeros(280)\n",
    "for i in range(4):\n",
    "    kaggle_ans[i*70:(i+1)*70] = i\n",
    "# remove last value\n",
    "kaggle_ans = kaggle_ans[:-1]\n",
    "\n",
    "# Convert predictions to labels\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
