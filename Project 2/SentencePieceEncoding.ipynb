{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Piece Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded and converted into a list of strings to be fed into the tokenizer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it to a list of strings\n",
    "df = pd.read_csv('Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()  # Assuming 'body' is the column containing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of housekeeping tasks have to be arranged in order for proper usage of the SentencePiece library. The list of corpus text is save to a temp file for dataloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corpus to a temporary file\n",
    "with open('temp_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in corpus:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will be trained on the full train set according to a vocabulary size of 5000 and will output a 'model' that can then be used to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = 'Tokenizers/sp_model'\n",
    "vocab_size = 5000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f'--input=temp_corpus.txt --model_prefix={output_model} --vocab_size={vocab_size}',\n",
    ")\n",
    "\n",
    "# Remove the temporary file\n",
    "import os\n",
    "os.remove('temp_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test how the tokenizer interprets the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁c', \"'\", 'est', '▁t', 'an', 'nant', '▁quel', '▁point', '▁quartier', 's', '▁centr', 'aux', '▁déj', 'à', '▁bien', '▁dess', 'er', 'vis', '▁transport', '▁act', 'if', '▁collectif', '▁continue', 'nt', '▁a', 'voir', '▁amélior', 'ations', '▁l', 'eurs', '▁infrastructure', 's', '▁p', 'endant', '▁quartier', 's', '▁péri', 'ph', 'éri', 'ques', '▁laissé', 's', '▁ja', 'ch', 'ère', '...', '▁c', \"'\", 'est', '▁rendu', '▁c', \"'\", 'est', '▁beaucoup', '▁plus', '▁facile', '▁viv', 're', '▁san', 's', '▁auto', '▁longue', 'uil', '▁bro', 's', 's', 'ard', '▁qu', \"'\", 'à', '▁st', '▁', 'laurent', '▁c', 'd', 'n', '▁n', 'd', 'g', '▁la', 's', 'alle', '▁montréal', '▁montréal', '▁nor', 'd', '▁etc', '▁comprend', 's', '▁qu', \"'\", 'il', '▁faut', '▁commence', 'r', '▁quelque', '▁part', '▁dis', '▁c', \"'\", 'est', '▁correct', '▁proc', 'é', 'der', '▁a', 'insi', '▁sach', 'ant', '▁service', '▁san', 's', '▁doute', '▁amélior', 'é', '▁', 'agrandi', '▁cha', 'que', '▁année', '▁c', \"'\", 'est', '▁frustra', 'nt', '▁pare', 'il', '▁', 'voir', '▁quel', '▁point', '▁chose', 's', '▁progress', 'ent', '▁l', 'ent', 'ement']\n"
     ]
    }
   ],
   "source": [
    "# Import the model that was just trained\n",
    "model_path = 'Tokenizers/sp_model.model'\n",
    "sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "# Encode a sample sentence from the dataset\n",
    "tokens = sp.encode_as_pieces(df['body'][543])\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
