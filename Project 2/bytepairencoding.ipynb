{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment a BPE tokenizer will be trained on the cleaned text found in the 'body' column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, pre_tokenizers, models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded and converted into a list of strings to be fed into the tokenizer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it to a list of strings\n",
    "df = pd.read_csv('Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()  # Assuming 'body' is the column containing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is trained in a custom manner such that the vocabulary reaches 10,000 items, it is aware of the alphabet, and some special tokens are included for exceptional cases in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initizalize the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize training\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=10000, \n",
    "    show_progress=True, \n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(), \n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('Tokenizers/bpe_tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize how the tokenizer works by outputting a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġc', \"'\", 'est', 'Ġtan', 'nant', 'Ġquel', 'Ġpoint', 'Ġquartiers', 'Ġcentr', 'aux', 'ĠdÃ©jÃł', 'Ġbien', 'Ġdess', 'erv', 'is', 'Ġtransport', 'Ġact', 'if', 'Ġcollect', 'if', 'Ġcontinu', 'ent', 'Ġavoir', 'ĠamÃ©li', 'orations', 'Ġleurs', 'Ġinfr', 'astruct', 'ures', 'Ġpendant', 'Ġquartiers', 'ĠpÃ©ri', 'ph', 'Ã©ri', 'ques', 'Ġlaiss', 'Ã©s', 'Ġj', 'ach', 'Ã¨re', 'âĢ¦', 'Ġc', \"'\", 'est', 'Ġrendu', 'Ġc', \"'\", 'est', 'Ġbeaucoup', 'Ġplus', 'Ġfacile', 'Ġvivre', 'Ġsans', 'Ġauto', 'Ġlongueuil', 'Ġbrossard', 'Ġqu', \"'\", 'Ãł', 'Ġst', 'Ġlaurent', 'Ġcd', 'n', 'Ġndg', 'Ġlas', 'alle', 'ĠmontrÃ©al', 'ĠmontrÃ©al', 'Ġnord', 'Ġetc', 'Ġcomprends', 'Ġqu', \"'\", 'il', 'Ġfaut', 'Ġcommencer', 'Ġquelque', 'Ġpart', 'Ġdis', 'Ġc', \"'\", 'est', 'Ġcorrect', 'Ġproc', 'Ã©der', 'Ġainsi', 'Ġsachant', 'Ġservice', 'Ġsans', 'Ġdoute', 'ĠamÃ©li', 'orÃ©', 'Ġag', 'rand', 'i', 'Ġchaque', 'ĠannÃ©e', 'Ġc', \"'\", 'est', 'Ġfrustr', 'ant', 'Ġpareil', 'Ġvoir', 'Ġquel', 'Ġpoint', 'Ġchoses', 'Ġprogress', 'ent', 'Ġlent', 'ement']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "encoded = tokenizer.encode(df['body'][543])\n",
    "print(encoded.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
