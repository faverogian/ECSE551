{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment a BPE tokenizer will be trained on the cleaned text found in the 'body' column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, pre_tokenizers, models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded and converted into a list of strings to be fed into the tokenizer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it to a list of strings\n",
    "df = pd.read_csv('Datasets/train_cleaned.csv')\n",
    "corpus = df['body'].tolist()  # Assuming 'body' is the column containing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is trained in a custom manner on the full train set such that the vocabulary reaches 10,000 items, it is aware of the alphabet, and some special tokens are included for exceptional cases in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initizalize the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize training\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=5000, \n",
    "    show_progress=True, \n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(), \n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('Tokenizers/bpe_tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize how the tokenizer works by outputting a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġc', \"'\", 'est', 'Ġtan', 'n', 'ant', 'Ġquel', 'Ġpoint', 'Ġquartiers', 'Ġcentr', 'aux', 'ĠdÃ©jÃł', 'Ġbien', 'Ġdess', 'erv', 'is', 'Ġtransport', 'Ġact', 'if', 'Ġcollect', 'if', 'Ġcontinu', 'ent', 'Ġavoir', 'Ġam', 'Ã©li', 'or', 'ations', 'Ġleurs', 'Ġinfr', 'astruct', 'ures', 'Ġpendant', 'Ġquartiers', 'Ġp', 'Ã©ri', 'ph', 'Ã©ri', 'ques', 'Ġlaiss', 'Ã©s', 'Ġj', 'ach', 'Ã¨re', 'âĢ¦', 'Ġc', \"'\", 'est', 'Ġrendu', 'Ġc', \"'\", 'est', 'Ġbeaucoup', 'Ġplus', 'Ġfacile', 'Ġvivre', 'Ġsans', 'Ġauto', 'Ġlongue', 'uil', 'Ġbro', 'ss', 'ard', 'Ġqu', \"'\", 'Ãł', 'Ġst', 'Ġlaurent', 'Ġcd', 'n', 'Ġn', 'd', 'g', 'Ġl', 'as', 'alle', 'ĠmontrÃ©al', 'ĠmontrÃ©al', 'Ġnord', 'Ġetc', 'Ġcomprends', 'Ġqu', \"'\", 'il', 'Ġfaut', 'Ġcommenc', 'er', 'Ġquelque', 'Ġpart', 'Ġdis', 'Ġc', \"'\", 'est', 'Ġcorrect', 'Ġproc', 'Ã©d', 'er', 'Ġa', 'ins', 'i', 'Ġsach', 'ant', 'Ġservice', 'Ġsans', 'Ġdoute', 'Ġam', 'Ã©li', 'orÃ©', 'Ġag', 'r', 'and', 'i', 'Ġchaque', 'ĠannÃ©e', 'Ġc', \"'\", 'est', 'Ġfr', 'ustr', 'ant', 'Ġpareil', 'Ġvoir', 'Ġquel', 'Ġpoint', 'Ġchoses', 'Ġpro', 'gress', 'ent', 'Ġl', 'ent', 'ement']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "encoded = tokenizer.encode(df['body'][543])\n",
    "print(encoded.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
